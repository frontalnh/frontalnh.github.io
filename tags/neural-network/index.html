<!DOCTYPE html>
<html class="has-navbar-fixed-top">

<head><meta name="generator" content="Hexo 3.8.0">
    <meta charset="utf-8">
<title>Tag: neural network - Jake.Lee&#39;s Blog</title>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="google-site-verification" content="PgpnJMuvO-IqYWyFljnyr-tusLhJUz1VRMmECenJyHE">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css">






<link rel="icon" href="/favicon.png">


<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ovo|Source+Code+Pro">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/bulma/0.6.2/css/bulma.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/css/justifiedGallery.min.css">


<link rel="stylesheet" href="/css/style.css">
<script defer src="//use.fontawesome.com/releases/v5.0.8/js/all.js"></script>

</head>

<body>
    
<nav class="navbar is-transparent is-fixed-top navbar-main" role="navigation" aria-label="main navigation">
    <div class="container">
        <div class="navbar-brand">
            <a class="navbar-item navbar-logo" href="/">
                
                <img src="/images/logo.png" alt="" height="28">
                
            </a>
            <div class="navbar-burger">
                <span></span>
                <span></span>
                <span></span>
            </div>
        </div>
        
        <div class="navbar-menu navbar-start">
            
            <a class="navbar-item " href="/archives">Archives</a>
            
            <a class="navbar-item " href="/categories">Categories</a>
            
            <a class="navbar-item " href="/tags">Tags</a>
            
            <a class="navbar-item " href="/about">About</a>
            
        </div>
        
        <div class="navbar-menu navbar-end">
            
            <a class="navbar-item search" href="javascript:;">
                <i class="fas fa-search"></i>
            </a>
            
            
            
            <a class="navbar-item" href="https://github.com/frontalnh">
                
                <i class="fab fa-github"></i>
                
            </a>
               
            
        </div>
    </div>
</nav>

        <section class="section section-heading">
    <div class="container">
        <div class="content">
            <h5>#neural network</h5>
        </div>
    </div>
</section>
<section class="section">
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
  (adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-6354931397950820",
    enable_page_level_ads: true
  });
</script>

    <div class="container">
    
        <article class="article content gallery" itemscope="" itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2017/12/09/Neural-network-back-propagation/" itemprop="url">
                Neural network back propagation
            </a>
            
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            <time datetime="2017-12-09T04:15:10.000Z" itemprop="datePublished">
                3 years ago
            </time>
        </span>
        
                
                    <span class="column is-narrow">
                        
                            
                                4 minutes read (About 653 words)
                    </span>
                    
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
        
                    <!-- toc -->
<ul>
<li><a href="#back-propagation-이란">back propagation 이란?</a></li>
<li><a href="#back-propagation-algorithm">Back propagation Algorithm</a></li>
<li><a href="#작동-원리">작동 원리</a></li>
<li><a href="#bpn-learning-processencoding">BPN Learning Process(Encoding)</a></li>
<li><a href="#bpn-retrieving-processdecoding">BPN Retrieving Process(Decoding)</a></li>
<li><a href="#bpn">BPN</a></li>
<li><a href="#gdrgeneralized-delta-rule">GDR(Generalized Delta Rule)</a></li>
<li><a href="#bpn-learning-algorithm">BPN Learning Algorithm</a></li>
</ul>
<!-- tocstop -->
<h2><span id="back-propagation-이란">back propagation 이란?</span></h2><p>multi layer perceptron에서의 학습 알고리즘 이다.</p>
<h2><span id="back-propagation-algorithm">Back propagation Algorithm</span></h2><p>다음과 같은 이유로  multi layer perceptron에서 BP를 사용한다.</p>
<ol>
<li>넓은 입력공간에서 복잡한 패턴을 가진다.</li>
<li>병렬 처리를 통해 빠른 계산속도를 가진다.</li>
<li>adaption을 위해 error back propagation을 한다.</li>
</ol>
<p>Back propagation을 하는 MLP의 구조를 BPN(Back Propagation Network)라고 한다.</p>
<h2><span id="작동-원리">작동 원리</span></h2><p>BPN은 다음과 같은 복잡한 패턴 매칭 문제를 해결할 수 있다.</p>
<ol>
<li>입출력 쌍 패턴 분석을 propagation adoption 방식을 사용하고 학습할 수 있다.</li>
<li>입력 자극이 hidden layer로 그리고 output layer 로 전달된다.</li>
</ol>
<p>다음과 같은 순서로 학습이 이루어 진다.</p>
<ol>
<li>패턴 입력이 인풋으로 주어짐</li>
<li>히든 레이어로 전달된</li>
<li>출력 레이어로 전달되고 출력 패턴이 나옴</li>
<li>desired pattern에서 output pattern의 차를 통해 error를 검출</li>
<li>검출된 error가 출력 레이어로 부터 히든 레이어로 전달되고 다시 입력단으로 전달됨</li>
<li>히든 레이어에서 에러의 상대적 기여도가 사용되고, 모든 노드에 대해 수행됨</li>
<li>connection strength가 업데이트 되고, 트레이닝 패턴이 다시 encode됨</li>
</ol>
<h2><span id="bpn-learning-processencoding">BPN Learning Process(Encoding)</span></h2><p>히든 레이어의 노드가 모든 입력 패턴을 인식할 수 있도록 스스로 재구성됨</p>
<h2><span id="bpn-retrieving-processdecoding">BPN Retrieving Process(Decoding)</span></h2><ol>
<li>입력된 입력패턴이 학습된 패턴과 같거나 유사하면 output layer에 나타남</li>
<li>입력된 입력 패턴이 학습된 데이터가 아니면, 히든 레이어에서 자극값을 생성하는 것이 억제됨</li>
</ol>
<h2><span id="bpn">BPN</span></h2><p>Back propagation 알고리즘의 핵심은 error가 weight를 스스로 업데이트 하는 것이다.<br>error BP는 지도 학습(supervised learning)의 핵심이다.<br>여러개의 입출력 쌍을 통해 출력을 예상할 수 있다.</p>
<p>Mapping network<br>:input과 output을 계산하는 함수</p>
<p><img src="\images\bpnImage.png" alt="bpn image"></p>
<h2><span id="gdrgeneralized-delta-rule">GDR(Generalized Delta Rule)</span></h2><p>입력 벡터를 통해 i 번체 히든 노드의 입력의 합을 구한다.</p>
<p><img src="\images\gdnImage.png" alt="gdr image"></p>
<p><img src="\images\gdn2.png" alt="gdr2"></p>
<p>output of th k^th output unit<br><img src="\images\outputOfKthOutputUnit.png" alt="output of kth output unit"></p>
<h2><span id="bpn-learning-algorithm">BPN Learning Algorithm</span></h2><ol>
<li>적용된 input으로 부터 전달된 신호를 기반으로 관련 출력을 계산하라</li>
<li>에러를 계산하라(desired-actual)</li>
<li>error 를 감소시키기 위한 weight update의 방향을 결정하라.</li>
<li>weight update의 양을 계산하라</li>
<li>weight를 업데이트 하라</li>
<li>1~5의 과정을 에러가 tolerance ranges 안에 들어갈 때 까지 반복하라.</li>
</ol>
<hr>

                        
    </div>
    
            
</article>



        
    
        <article class="article content gallery" itemscope="" itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2017/11/30/Neural-network-Bidirectional-Associative-Memory-BAM/" itemprop="url">
                Neural network Bidirectional Associative Memory(BAM)
            </a>
            
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            <time datetime="2017-11-29T17:07:12.000Z" itemprop="datePublished">
                3 years ago
            </time>
        </span>
        
                
                    <span class="column is-narrow">
                        
                            
                                a few seconds read (About 1 words)
                    </span>
                    
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
        
                    <!-- toc -->
<!-- tocstop -->

                        
    </div>
    
            
</article>



        
    
        <article class="article content gallery" itemscope="" itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2017/11/29/Neural-network-Perceptron과-ADALINE/" itemprop="url">
                Neural network Perceptron과 ADALINE
            </a>
            
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            <time datetime="2017-11-29T04:04:12.000Z" itemprop="datePublished">
                3 years ago
            </time>
        </span>
        
                
                    <span class="column is-narrow">
                        
                            
                                9 minutes read (About 1319 words)
                    </span>
                    
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
        
                    <!-- toc -->
<ul>
<li><a href="#perceptron-이란">Perceptron 이란?</a></li>
<li><a href="#photo-perceptron의-예">photo perceptron의 예</a></li>
<li><a href="#perception-convergence-theorem">Perception Convergence Theorem</a></li>
<li><a href="#problems-in-perceptron-single-layer-xor-문제">problems in perceptron (single layer) - XOR 문제</a></li>
<li><a href="#하이퍼-플레인과-hidden-node">하이퍼 플레인과 hidden node</a></li>
<li><a href="#multi-layer-perceptron을-이용한-xor-문제의-해결">multi layer perceptron을 이용한 XOR 문제의 해결</a></li>
<li><a href="#weight-vector는-왜-hyper-plane과-수직을-이루는가">weight vector는 왜 hyper plane과 수직을 이루는가?</a></li>
<li><a href="#adalineadaptive-linear-neuron이란-무엇이며-perceptron-과의-차이점은-무엇인가">ADALINE(ADAptive LInear NEuron)이란 무엇이며 perceptron 과의 차이점은 무엇인가?</a></li>
<li><a href="#adaline">ADALINE</a></li>
<li><a href="#least-mean-squareslms">Least Mean Squares(LMS)</a></li>
<li><a href="#rlmsrelative-least-mean-squares">RLMS(Relative Least Mean Squares)</a></li>
<li><a href="#neural-network-설계-프로세스">Neural network 설계 프로세스</a></li>
</ul>
<!-- tocstop -->
<h2><span id="perceptron-이란">Perceptron 이란?</span></h2><p>frank rosenblatt 박사가 1950년 발표한 내용으로 McCulloch-pitts 모델의 확장 개념이다.<br>통계적으로 구분된 이론이다.(확률적, 거시적)</p>
<h2><span id="photo-perceptron의-예">photo perceptron의 예</span></h2><p><img src="\images\photoPerceptron의예.png" alt="photo perceptron의 예"></p>
<h2><span id="perception-convergence-theorem">Perception Convergence Theorem</span></h2><p>두개의 패턴을 학습하는 과정에서, 유한번의 사이클 내에 두 개의 패턴을 구분할 수 있다.</p>
<p><img src="\images\perceptronLearningAlgorithm.png" alt="perceptron learning algorithm"></p>
<p>여기서 N과 P는 각각 Negative, Positive를 의미한다.<br>P와 N에서 랜덤하게 벡터를 뽑아서 Weight와 곱해준 값이<br>P의 경우 Positive 해 질때 까지 계속 덧셈을 하고<br>N의 경우 Negative 해 질때까지 계속 뺄셈을 하여 Positive 하고 Negative 함을 맞추어 준다.</p>
<h2><span id="problems-in-perceptron-single-layer-xor-문제">problems in perceptron (single layer) - XOR 문제</span></h2><p>single layer perceptron에서는 XOR 함수를 만족하는 weight를 찾을 수 없는 문제가 발생한다.</p>
<p><img src="\images\problemsInPerceptron.png" alt="problems in perceptron"></p>
<p><img src="\images\problemsInPerceptron2.png" alt="problems in perceptron2"></p>
<p><img src="\images\problemsInPerceptron3.png" alt="problems in perceptron3"></p>
<h2><span id="하이퍼-플레인과-hidden-node">하이퍼 플레인과 hidden node</span></h2><p>하이퍼 플레인이란 n차 공간에서 TLU(Threshold Logic Unit)역할을 하는 n-1차 공간이다.</p>
<p>hidden node란 xor 문제를 해결하는 mid-layer들을 의미한다.</p>
<p><img src="\images\하이퍼플레인과HiddenNode.png" alt="하이퍼 플레인과 hidden node"></p>
<h2><span id="multi-layer-perceptron을-이용한-xor-문제의-해결">multi layer perceptron을 이용한 XOR 문제의 해결</span></h2><p>멀티레이어 퍼셉트론을 이용하여 XOR 문제를 해결할 수 있다.<br><img src="\images\multiLayerPerceptronSolvingXORProblems.png" alt="multi layer perceptron solving XOR Problems"></p>
<h2><span id="weight-vector는-왜-hyper-plane과-수직을-이루는가">weight vector는 왜 hyper plane과 수직을 이루는가?</span></h2><p>weight vector는 hyper plane과 수직을 이룬다.</p>
<p><img src="\images\weightVectorIsPerpendicularToHyperPlane.png" alt="weight vector is perpendicular to hyper plane"></p>
<h2><span id="adalineadaptive-linear-neuron이란-무엇이며-perceptron-과의-차이점은-무엇인가">ADALINE(ADAptive LInear NEuron)이란 무엇이며 perceptron 과의 차이점은 무엇인가?</span></h2><p>ADALINE이란 adaptive signal processing을 수행한다.</p>
<p>다음과 같은 면에서 perceptron과 차이점을 가진다.</p>
<ol>
<li>dc 바이어스와 그에 대한 weight가 포함되어 있다.</li>
<li>perceptron은 0, +1의 출력을 가지는데 반해 -1 +1의 bipolar 출력을 가진다.</li>
<li>TLU가 단지 선형함수이다.</li>
</ol>
<p><img src="\images\perceptronAndADALINE.png" alt="perceptron and ADALINE"></p>
<h2><span id="adaline">ADALINE</span></h2><p>ALC: Adaptive Linear Combiner<br><img src="\images\adaline.png" alt="adaline"></p>
<h2><span id="least-mean-squareslms">Least Mean Squares(LMS)</span></h2><p>L개의 입력값을 통한 출력값과 L개의 desired output을 비교하여 mean of square error가 가장 작은 weights를 구한다.</p>
<p><img src="\images\aDALINELMS.png" alt="ADALINE LMS"></p>
<p>solution<br><img src="\images\aDALINELMSSolution.png" alt="ADALINE LMS Solution"></p>
<h2><span id="rlmsrelative-least-mean-squares">RLMS(Relative Least Mean Squares)</span></h2><p>학습되어야 할 데이터의 숫자가 증가하거나 벡터의 사이즈가 증가하는 경우에 문제가 생긴다.<br>모호한 방정식을 풀어야 하거나, 방정식의 개수가 weight보다 많은 경우 문제가 생긴다.<br>또한 inverse matrix를 구해야 하는 경우도 문제가 되는데, 데이터의 숫자가 많아짐에 따라 Determination이 0이 되는 값이 있을 수 있다.<br>이 경우 LMS를 줄여주는 방향으로 weight를 스스로 변화시키는 방법으로 weight를 조절해 갈 수 있는데 이를 RLMS라고 한다.</p>
<p><img src="\images\rLMSImage.png" alt="RLMS image"></p>
<p><img src="\images\rLMSProcess.png" alt="RLMS process"></p>
<h2><span id="neural-network-설계-프로세스">Neural network 설계 프로세스</span></h2><p>perceptron의 경우 TLU가 non-linear하기 때문에 계산하기 어려우므로 ADALINE을 이용한다.</p>
<p><em><strong>1. 학습되어야 할 데이터쌍이 몇개인가?</strong></em><br>문제의 복잡도에 따라 다르다. 학습되어야 할 데이터가 많을 수록 더 많은 learning cycle을 필요로 한다.<br>neural net의 저장용량에 따라 cross-talk 없이 효과적인 학습과 재현이 가능해야 한다.<br>최종적으로, incremental learning이 필요하다.</p>
<p><em><strong>2. 각 학습 입력쌍의 기대되는 출력값은 무엇인가?</strong></em><br>GMP(Genralized Modus Ponens)와 밀접한 관련이 있다.<br>학습 쌍이 학습 알고리즘 상의 weight에 저장되어 있다면, 같은 입력 패턴이 들어갔을ㄷ대 반드시 같은 결과 패턴이 나와야 한다.</p>
<p><em><strong>3. weight vector의 차원은 무엇이며 초기 추측값은 무엇인가?</strong></em><br>센서의 개수와 잘 정의된 입력값에 관계가 있다.<br>초기 추측값은 랜덤하게 선택하거나 대충 선택한다.</p>
<p><em><strong>4. bias weight이 필요한가? 필요하다면 언제 필요한가?</strong></em><br>어떤 필터에서는 필요가 없다.<br>hyper plane이 원점과 거리가 매우 멀다면 필요하다.</p>
<p><em><strong>5. pattern의 통계적 성질이 시간에 따라 변하면 어떠한가?</strong></em><br>일반적으로 시간에 따른 변수들의 통계적 특성이 불변한다고 가정한다.<br>만약에 변한다면 학습 주기는 처음부터 다시 시작되어야 한다.</p>
<p>incremental learning을 한다면 하나의 패턴쌍을 제거할 수 있는가? 어떤것이 변하는가?</p>
<p><em><strong>6. learning coefficient 적합한 값은 얼마인가?</strong></em><br>학습계수는 최저값과 최고값이 있어야 한다. 학습에서 이 크기는 매우매우 중요하다.<br>작은 학습계수는 학습의 수렴이 천천히 일어나게 하고, 큰 학습 계수는 가끔 매우 긴 수렴 주기로 가게 하거나 혹은 발산하게 할 수 있다.</p>
<p><img src="\images\learningCoefficient.png" alt="learning coefficient"></p>
<p><em><strong>7. learning cycle의 종료 규칙을 어떻게 정하는가?</strong></em><br>출력 조건과 관련이 있다. 만약 기대값과 실제 출력값이 허용치 안에서 계속 머문다면 학습 주기를 종료한다.</p>
<hr>

                        
    </div>
    
            
</article>



        
    
        <article class="article content gallery" itemscope="" itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2017/11/29/Neural-Network의-기본-개념/" itemprop="url">
                Neural Network의 기본 개념
            </a>
            
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            <time datetime="2017-11-29T02:49:33.000Z" itemprop="datePublished">
                3 years ago
            </time>
        </span>
        
                
                    <span class="column is-narrow">
                        
                            
                                8 minutes read (About 1216 words)
                    </span>
                    
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
        
                    <!-- toc -->
<ul>
<li><a href="#neural-network의-역사">neural network의 역사</a></li>
<li><a href="#왜-인공신경망이-각광을-받게-되었는가">왜 인공신경망이 각광을 받게 되었는가?</a></li>
<li><a href="#인공신경망이란">인공신경망이란?</a></li>
<li><a href="#생체-신경망과-컴퓨터-신경망의-연산속도">생체 신경망과 컴퓨터 신경망의 연산속도</a></li>
<li><a href="#문자의-인식">문자의 인식</a></li>
<li><a href="#기초적인-생체-신경망">기초적인 생체 신경망</a></li>
<li><a href="#neural-network-computation">Neural Network Computation</a></li>
<li><a href="#mcculloch-pitts-theoryold">McCulloch-Pitts Theory(old)</a></li>
<li><a href="#hebbian-learning">Hebbian Learning</a></li>
<li><a href="#general-processing-element">General Processing Element</a></li>
</ul>
<!-- tocstop -->
<h2><span id="neural-network의-역사">neural network의 역사</span></h2><p>1940년 후반, 디지털 컴퓨터의 출현</p>
<p>1950년 후반, perceptron과 ADALINE의 출현</p>
<p>1969년 ~ 1980년 초반, single layer perceptron에서 xor 문제 발생</p>
<p>1986년, 전환점을 맞이하게 된다.<br>multi layer perceptron이 xor 문제를 해결하다.</p>
<p>1990년, optical neural network, 디지털 하드웨어</p>
<h2><span id="왜-인공신경망이-각광을-받게-되었는가">왜 인공신경망이 각광을 받게 되었는가?</span></h2><ol>
<li>경험, 데이터, 실험을 통해 강력한 학습이 가능하다.</li>
<li>물체의 분류와 식별이 가능해진다.</li>
<li>빠른 병렬연산으로 이상적으로 빛의 속도로 연산이 가능하다.</li>
<li>잠재 지능이 존재한다.</li>
<li>수많은 범용 어플리케이션이 나오고 있다.</li>
</ol>
<h2><span id="인공신경망이란">인공신경망이란?</span></h2><p>인공 신경망이란 processing elements 로 구성된 여러개의 장치들이 connection strength or weight이 다른 여러 개의 연결망을 통해 연결된 복합체이다.<br>많은 패턴들과 수치 데이터들이 학습되고 저장될 수 있다.<br>여기서 weights란 인공 신경만이 학습하고 저장하기 위한 수치값들로 메모리에 저장되는 값이다.</p>
<p>adaption이란 예제와 실험들로 학습하는 것을 말한다.</p>
<h2><span id="생체-신경망과-컴퓨터-신경망의-연산속도">생체 신경망과 컴퓨터 신경망의 연산속도</span></h2><p>컴퓨터의 속도가 생체 신경망에 비해 10,000,000배나 더 빠른 속도를 보인다.<br>하지만 컴퓨터는 병렬적 처리를 하지 못하는데 반해 생체신경망은 막대한 병렬연산이 가능하고 복잡하게 연결되어 있기 때문에 컴퓨터가 100msec가 걸려야 풀 수 있는 한번의 인식을 단지 0.1msec 만에 해결한다.</p>
<h2><span id="문자의-인식">문자의 인식</span></h2><p>손으로 작성되었는가 혹은 컴퓨터로 타이핑 되었는가, 기울기 확대율, 회전율 등에 따라 다른 인식율을 보인다.</p>
<p>기본적인 패턴인식은 패턴을 픽셀단위로 분해아여 벡터화 시킨다.<br>인공 신경망의 node와 connection strength를 조절하여 분석한다.</p>
<p><img src="\images\characterRecognition.png" alt="character recognition"></p>
<h2><span id="기초적인-생체-신경망">기초적인 생체 신경망</span></h2><p>뉴런</p>
<ol>
<li>부분적인 투과성을 가진다.</li>
<li>potential 차이가 존재하며 이온들이 이동한다.</li>
</ol>
<p>이온들이 potential차가 평행을 이루는 지점까지 운동한다.</p>
<p><strong>시냅스</strong></p>
<p>시냅스는 Neuro-transmitter이다.<br>시냅스가 Ca(+)의 투과성을 높여주면 action potential이 증가한다.</p>
<p>투과성이 critical point를 넘기게 되면 action potential이 이동하여 다른 뉴런에 연결된다.</p>
<h2><span id="neural-network-computation">Neural Network Computation</span></h2><blockquote>
<p>뇌는 수많은 연결을 통해 어떻게 엄청난 능력을 가지게 되었는가?</p>
</blockquote>
<h2><span id="mcculloch-pitts-theoryold">McCulloch-Pitts Theory(old)</span></h2><ol>
<li>뉴런의 활동은 일어나거나 일어나지 않거나 두가지 상태를 가진다. 중간은 없다.</li>
<li>신경이 흥분되려면 두 개 이상의 시냅스가 흥분되어야 한다.</li>
<li>delay는 오직 스냅스에서만 발생한다.</li>
<li>억제 시냅스는 뉴런 활동성을 저해한다.</li>
<li>연결망의 구조는 시간에 따라 변하지 않는다.</li>
</ol>
<blockquote>
<p>뉴런은 단순한 장치이지만, 적절하게 연결되어 신경 시스템을 구성하면 막대한 계산이 가능하다.</p>
</blockquote>
<h2><span id="hebbian-learning">Hebbian Learning</span></h2><blockquote>
<p>생체신경망은 최초에 직렬적으로 작동할 능력과 지식을 갖추지 못했다.</p>
</blockquote>
<p><strong>Pavlov’s Conditioned Response</strong></p>
<p>뉴런 A 의 축색돌기가 뉴런 B를 자극하기에 충분하고 지속적으로 계속적으로 자극이 발생한다면 하나 혹은 여러개의 뉴런에서 성장 혹은 신진대사가 이루어 지고 뉴런 C의 효율이 높아진다.<br>그리고 마침내 뉴런 C를 자극하는 다른 뉴런인 뉴런 B가 증가하게 된다.</p>
<p><img src="\images\파블로프의모델.png" alt="파블로프의 모델"></p>
<p><img src="\images\파블로프의조건적반응.png" alt="파블로프의 조건적 반응"></p>
<p>Hebb’s assumption says, “The excitation of neuron A has enough stimulus for neuron B, but that of neuron C doesn’t have. Now, with neuron A, the connection strength of neuron C increases, and eventually, without neuron A, neuron B can be excited by neuron C only.”</p>
<h2><span id="general-processing-element">General Processing Element</span></h2><p>인공 신경세포를 노드 혹은 유닛으로 나타내는 단위이다.</p>
<p><img src="\images\pE의개념도.png" alt="PE의 개념도"></p>
<p><img src="\images\pE를이루는기본공식.png" alt="PE를 이루는 기본 공식"></p>
<p>수학에서 뉴럴 네트워크의 설계는 시간에 따라 변하는 역동적인 시스템이다.</p>
<p><strong>학습 법칙</strong><br>connection strength는 학습이 진행됨에 따라 업데이트되며, data of weights가 무엇을 배우지에 관한 정보를 기록하게 된다.</p>
<p>vector notations<br><img src="\images\pE모델링을위한벡터표기.png" alt="PE 모델링을 위한 벡터 표기"></p>
<hr>

                        
    </div>
    
            
</article>



        
    
    
    </div>
</section>

            <footer class="footer">
    <div class="container">
        <div class="columns content">
            <div class="column is-narrow has-text-centered">
                &copy; 2020 Jake.Lee 이남훈&nbsp;
                Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> & <a href="http://github.com/ppoffice/hexo-theme-minos">Minos</a>
            </div>
            <div class="column is-hidden-mobile"></div>

            
            <div class="column is-narrow">
                <div class="columns is-mobile is-multiline is-centered">
                
                    
                <a class="column is-narrow has-text-black" href="https://github.com/frontalnh">
                    
                    GitHub
                    
                </a>
                
                </div>
            </div>
            
            
        </div>
    </div>
</footer>
                <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>

<!-- test if the browser is outdated -->
<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js"></script>
<script>
    $(document).ready(function () {
        //plugin function, place inside DOM ready function
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        })
    });
</script>

<script>
    window.FontAwesomeConfig = {
        searchPseudoElements: true
    }
</script>


    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script>
    MathJax.Hub.Config({
        "HTML-CSS": {matchFontHeight: false},
        SVG: {matchFontHeight: false},
        CommonHTML: {matchFontHeight: false}
    });
</script>

    
    
<script src="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/js/lightgallery-all.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/js/jquery.justifiedGallery.min.js"></script>
<script>
    (function ($) {
        $(document).ready(function () {
            if (typeof($.fn.lightGallery) === 'function') {
                $('.article.gallery').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof($.fn.justifiedGallery) === 'function') {
                $('.justified-gallery').justifiedGallery();
            }
        });
    })(jQuery);
</script>

    
    
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110077250-2"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-110077250-2');
</script>


    


<script src="/js/script.js"></script>

                    
                        <div class="searchbox ins-search">
    <div class="searchbox-mask"></div>
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="Type something...">
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: 'Posts',
                PAGES: 'Pages',
                CATEGORIES: 'Categories',
                TAGS: 'Tags',
                UNTITLED: '(Untitled)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/js/insight.js"></script>
                            
</body>

</html>